"""Study module providing the central state container for an XBrainLab experiment."""

from __future__ import annotations

from .controller.dataset_controller import DatasetController
from .controller.evaluation_controller import EvaluationController
from .controller.preprocess_controller import PreprocessController
from .controller.training_controller import TrainingController
from .controller.visualization_controller import VisualizationController
from .data_manager import DataManager
from .dataset import Dataset, DatasetGenerator, DataSplittingConfig, Epochs
from .load_data import Raw, RawDataLoader
from .preprocessor import PreprocessBase
from .training import ModelHolder, Trainer, TrainingOption
from .training_manager import TrainingManager
from .utils import validate_type
from .utils.logger import logger


class Study:
    """Class for storing required info a study.

    Attributes:
        loaded_data_list: list[:class:`XBrainLab.backend.load_data.Raw`].
            The raw data result loaded by
            :class:`XBrainLab.backend.load_data.RawDataLoader`.
        preprocessed_data_list: list[:class:`XBrainLab.backend.load_data.Raw`].
            The preprocessed data result generated by
                sequences of
                :class:`XBrainLab.backend.preprocessor.base.PreprocessBase`.
        epoch_data: :class:`XBrainLab.backend.dataset.Epochs` or None.
            The epoch data generated from list of
            :class:`XBrainLab.backend.load_data.Raw`.
        datasets: list[:class:`XBrainLab.backend.dataset.Dataset`].
            The datasets generated from
            :class:`XBrainLab.backend.dataset.DatasetGenerator`.
        model_holder: :class:`XBrainLab.backend.training.ModelHolder` or None.
            The model with parameters.
        training_option: :class:`XBrainLab.backend.training.TrainingOption` or None.
            The training option.
        trainer: :class:`XBrainLab.backend.training.Trainer` or None.
            The model trainer.

    """

    def __init__(self) -> None:
        self.data_manager = DataManager()
        self.training_manager = TrainingManager()

        # Controller cache for singleton-like access
        self._controllers: dict = {}

        logger.info("Study initialized (with DataManager + TrainingManager)")

    # --- DataManager Delegation Properties ---
    # NOTE: Setters write directly to DataManager attributes for backward
    # compatibility and test convenience. Production code should use the
    # dedicated set_* / clean_* methods which include validation.

    @property
    def loaded_data_list(self) -> list[Raw]:
        return self.data_manager.loaded_data_list

    @loaded_data_list.setter
    def loaded_data_list(self, value: list[Raw]) -> None:
        self.data_manager.loaded_data_list = value

    @property
    def preprocessed_data_list(self) -> list[Raw]:
        return self.data_manager.preprocessed_data_list

    @preprocessed_data_list.setter
    def preprocessed_data_list(self, value: list[Raw]) -> None:
        self.data_manager.preprocessed_data_list = value

    @property
    def epoch_data(self) -> Epochs | None:
        return self.data_manager.epoch_data

    @epoch_data.setter
    def epoch_data(self, value: Epochs | None) -> None:
        self.data_manager.epoch_data = value

    @property
    def datasets(self) -> list[Dataset]:
        return self.data_manager.datasets

    @datasets.setter
    def datasets(self, value: list[Dataset]) -> None:
        self.data_manager.datasets = value

    @property
    def dataset_generator(self):
        return self.data_manager.dataset_generator

    @dataset_generator.setter
    def dataset_generator(self, value):
        self.data_manager.dataset_generator = value

    @property
    def dataset_locked(self) -> bool:
        return self.data_manager.dataset_locked

    # --- TrainingManager Delegation Properties ---
    # Same backward-compat pattern as DataManager: direct attribute access
    # delegated to TrainingManager so controllers/tests continue to work.

    @property
    def model_holder(self) -> ModelHolder | None:
        return self.training_manager.model_holder

    @model_holder.setter
    def model_holder(self, value: ModelHolder | None) -> None:
        self.training_manager.model_holder = value

    @property
    def training_option(self) -> TrainingOption | None:
        return self.training_manager.training_option

    @training_option.setter
    def training_option(self, value: TrainingOption | None) -> None:
        self.training_manager.training_option = value

    @property
    def trainer(self) -> Trainer | None:
        return self.training_manager.trainer

    @trainer.setter
    def trainer(self, value: Trainer | None) -> None:
        self.training_manager.trainer = value

    @property
    def saliency_params(self) -> dict | None:
        return self.training_manager.saliency_params

    @saliency_params.setter
    def saliency_params(self, value: dict | None) -> None:
        self.training_manager.saliency_params = value

    # --- Pipeline Stage ---

    @property
    def pipeline_stage(self):
        """Compute the current pipeline stage from live state.

        Returns:
            The :class:`~XBrainLab.llm.pipeline_state.PipelineStage`
            derived from the current Study attributes.  This is always
            recomputed — never cached — so it cannot drift from reality.

        """
        from XBrainLab.llm.pipeline_state import compute_pipeline_stage  # noqa: PLC0415

        return compute_pipeline_stage(self)

    # --- Controller Access ---
    def get_controller(self, controller_type: str):
        """Get or create a cached controller instance.

        Args:
            controller_type: One of ``"dataset"``, ``"preprocess"``,
                ``"training"``, ``"evaluation"``, or ``"visualization"``.

        Returns:
            The controller instance for the given type.

        Raises:
            ValueError: If the controller type is unknown.

        """
        if controller_type not in self._controllers:
            if controller_type == "dataset":
                self._controllers[controller_type] = DatasetController(self)
            elif controller_type == "preprocess":
                self._controllers[controller_type] = PreprocessController(self)
            elif controller_type == "training":
                self._controllers[controller_type] = TrainingController(self)
            elif controller_type == "evaluation":
                self._controllers[controller_type] = EvaluationController(self)
            elif controller_type == "visualization":
                self._controllers[controller_type] = VisualizationController(self)
            else:
                raise ValueError(f"Unknown controller type: {controller_type}")
        return self._controllers[controller_type]

    # step 1 - load data
    def get_raw_data_loader(self) -> RawDataLoader:
        """Get the raw data loader instance from DataManager."""
        return self.data_manager.get_raw_data_loader()

    def backup_loaded_data(self) -> None:
        """Backup the currently loaded data list via DataManager."""
        self.data_manager.backup_loaded_data()

    def set_loaded_data_list(
        self,
        loaded_data_list: list[Raw],
        force_update: bool = False,
    ) -> None:
        """Set loaded data list in DataManager.

        Cleans trainer first since new raw data invalidates it.
        """
        self.clean_trainer(force_update=force_update)
        self.data_manager.set_loaded_data_list(loaded_data_list, force_update)

    # step 2 - preprocess
    def set_preprocessed_data_list(
        self,
        preprocessed_data_list: list[Raw],
        force_update: bool = False,
    ) -> None:
        """Set preprocessed data list in DataManager."""
        self.data_manager.set_preprocessed_data_list(
            preprocessed_data_list,
            force_update,
        )

    def reset_preprocess(self, force_update=False) -> None:
        """Reset preprocessing via DataManager."""
        self.data_manager.reset_preprocess(force_update)

    def preprocess(self, preprocessor: type[PreprocessBase], **kwargs) -> None:
        """Apply a preprocessing step via DataManager.

        Args:
            preprocessor: The preprocessor class to apply.
            **kwargs: Keyword arguments forwarded to the preprocessor.

        """
        self.data_manager.preprocess(preprocessor, **kwargs)

    # step 3 - split data for training
    def get_datasets_generator(self, config: DataSplittingConfig) -> DatasetGenerator:
        """Create a dataset generator based on current epoch data.

        Args:
            config: Data splitting configuration.

        Returns:
            A dataset generator ready to produce train/val/test splits.

        Raises:
            ValueError: If no epoch data is available.

        """
        if not self.epoch_data:
            raise ValueError("No valid epoch data is generated")
        validate_type(config, DataSplittingConfig, "config")
        return DatasetGenerator(self.epoch_data, config)

    def set_datasets(self, datasets: list[Dataset], force_update: bool = False) -> None:
        """Set datasets in DataManager and clean trainer."""
        # Clean trainer is Study responsibility as it holds trainer
        self.clean_trainer(force_update=force_update)
        self.data_manager.set_datasets(datasets, force_update)

    # step 4 - training config
    def set_training_option(
        self,
        training_option: TrainingOption,
        force_update: bool = False,
    ) -> None:
        """Set training option via TrainingManager."""
        self.training_manager.set_training_option(training_option, force_update)

    def set_model_holder(
        self,
        model_holder: ModelHolder,
        force_update: bool = False,
    ) -> None:
        """Set model holder via TrainingManager."""
        self.training_manager.set_model_holder(model_holder, force_update)

    def generate_plan(self, force_update: bool = False, append: bool = False) -> None:
        """Generate training plan via TrainingManager.

        Args:
            force_update: Whether to clear existing plan.
            append: Whether to append to existing plan.

        """
        self.training_manager.generate_plan(
            self.datasets,
            force_update=force_update,
            append=append,
        )

    # step 5 - training
    def train(self, interact: bool = False) -> None:
        """Start training process via TrainingManager."""
        self.training_manager.train(interact=interact)

    def stop_training(self) -> None:
        """Stop training execution via TrainingManager."""
        self.training_manager.stop_training()

    def is_training(self) -> bool:
        """Return whether training is currently running."""
        return self.training_manager.is_training()

    # step 6 - evaluation
    def export_output_csv(self, filepath: str, plan_name: str, real_plan_name: str):
        """Export model inference output to csv file via TrainingManager."""
        self.training_manager.export_output_csv(filepath, plan_name, real_plan_name)

    # step 7 - visualization
    def set_channels(self, chs: list[str], positions: list[tuple]) -> None:
        """Set channels and positions for visualization."""
        if not self.epoch_data:
            raise ValueError("No valid epoch data is generated")
        self.epoch_data.set_channels(chs, positions)

    def get_saliency_params(self) -> dict | None:
        """Return parameters for saliency computation via TrainingManager."""
        return self.training_manager.get_saliency_params()

    def set_saliency_params(self, saliency_params) -> None:
        """Set saliency parameters via TrainingManager."""
        self.training_manager.set_saliency_params(saliency_params)

    # --- Clean Workflow ---
    # Study extends DataManager's cleaning by adding trainer awareness.

    def lock_dataset(self) -> None:
        """Lock dataset via DataManager."""
        self.data_manager.lock_dataset()

    def unlock_dataset(self) -> None:
        """Unlock dataset via DataManager."""
        self.data_manager.unlock_dataset()

    def is_locked(self) -> bool:
        """Check if dataset is locked via DataManager."""
        return self.data_manager.is_locked()

    def has_raw_data(self) -> bool:
        """Return whether raw data or downstream state exists (pure query)."""
        return self.data_manager.has_raw_data() or self.has_trainer()

    def has_datasets(self) -> bool:
        """Return whether datasets or a trainer exist (pure query)."""
        return self.data_manager.has_datasets() or self.has_trainer()

    def has_trainer(self) -> bool:
        """Return whether a trainer is configured (pure query)."""
        return self.training_manager.has_trainer()

    def clean_raw_data(self, force_update: bool = True) -> None:
        """Clean raw data and all downstream state including trainer."""
        self.clean_datasets(force_update=force_update)
        self.data_manager.clean_raw_data(force_update)

    def clean_datasets(self, force_update: bool = True) -> None:
        """Clean datasets and trainer."""
        self.clean_trainer(force_update=force_update)
        self.data_manager.clean_datasets(force_update)

    def clean_trainer(self, force_update: bool = True) -> None:
        """Clean the trainer via TrainingManager."""
        self.training_manager.clean_trainer(force_update=force_update)
